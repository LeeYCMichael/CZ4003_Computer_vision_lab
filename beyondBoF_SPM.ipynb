{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_list, train_label_list = [],  []\n",
    "test_img_list, test_label_list = [],  []\n",
    "\n",
    "folder_dir = './Lab2/101_ObjectCategories'\n",
    "for j, (root, dirs, file) in enumerate(os.walk(folder_dir)):\n",
    "\n",
    "    for k, name in enumerate(dirs): \n",
    "        if name == \"BACKGROUND_Google\": # irrelevant class\n",
    "            continue    \n",
    "        \n",
    "        filepath = folder_dir + \"/\" + name\n",
    "        for i, f in enumerate(os.listdir(filepath)):\n",
    "            if i > 80:\n",
    "                break\n",
    "\n",
    "            img_path = os.path.join(folder_dir, name, f)\n",
    "            label = name\n",
    "            \n",
    "            if i < 30: # following the paper, we take 30 for train set and 50 for test. If there are are less than 50 for test, it doesnt matter (according to the paper)\n",
    "                train_img_list.append(cv2.imread(img_path))\n",
    "                train_label_list.append(label)\n",
    "            else:\n",
    "                test_img_list.append(cv2.imread(img_path))  \n",
    "                test_label_list.append(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Features (BoF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes sift descriptor for imgs\n",
    "def compute_sift_descriptor(imgs):\n",
    "    \n",
    "    step_size = 15\n",
    "    sift_list = []\n",
    "    for img in imgs:\n",
    "        # sift = cv2.SIFT_create()\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        kp = [cv2.KeyPoint(x, y, step_size) for y in range(0, img.shape[0], step_size) \n",
    "                                        for x in range(0, img.shape[1], step_size)]\n",
    "        \n",
    "        dense_feat = sift.compute(img, kp)\n",
    "        sift_list.append(dense_feat[1])\n",
    "    return sift_list\n",
    "\n",
    "def get_clusters(num_clusters, X_train_desc):\n",
    "    k_means = KMeans(n_clusters=num_clusters, random_state=SEED).fit(X_train_desc)\n",
    "    \n",
    "    return k_means\n",
    "\n",
    "def get_histogram(X_train, k_means_classifier, num_clusters):\n",
    "    hist_list = []\n",
    "    for i, img in enumerate(X_train):\n",
    "        features = k_means_classifier.predict(img)\n",
    "        hist = np.bincount(features, minlength = num_clusters).reshape(1,-1).flatten()\n",
    "        hist_list.append(hist)\n",
    "        \n",
    "    return hist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\micha\\miniconda3\\envs\\michael\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    }
   ],
   "source": [
    "X_train = compute_sift_descriptor(train_img_list)\n",
    "X_test = compute_sift_descriptor(test_img_list)\n",
    "\n",
    "X_train_desc = []\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(X_train[i].shape[0]):\n",
    "        X_train_desc.append(X_train[i][j,:])\n",
    "\n",
    "X_train_desc = np.array(X_train_desc)\n",
    "\n",
    "num_clusters = 60\n",
    "k_means_classifier = get_clusters(num_clusters, X_train_desc)\n",
    "\n",
    "X_train_hist = get_histogram(X_train, k_means_classifier, num_clusters)\n",
    "X_test_hist = get_histogram(X_test, k_means_classifier, num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy for BoF: 33.771%\n"
     ]
    }
   ],
   "source": [
    "# normalize histograms\n",
    "scaler = StandardScaler().fit(X_train_hist)\n",
    "train_hist = scaler.transform(X_train_hist)\n",
    "test_hist = scaler.transform(X_test_hist)\n",
    "\n",
    "# SVM\n",
    "clf = LinearSVC(random_state=SEED)\n",
    "clf.fit(train_hist, train_label_list)\n",
    "pred = clf.predict(test_hist)\n",
    "accuracy = np.mean(pred == test_label_list)*100\n",
    "\n",
    "print(f\"Classification Accuracy for BoF: {accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Pyramid Mapping (SPM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes sift descriptor for imgs\n",
    "def compute_sift_descriptor_grid(img):\n",
    "    \n",
    "    step_size = 2\n",
    "    # sift = cv2.SIFT_create()\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp = [cv2.KeyPoint(x, y, step_size) for y in range(0, img.shape[0], step_size) \n",
    "                                    for x in range(0, img.shape[1], step_size)]\n",
    "    \n",
    "    dense_feat = sift.compute(img, kp)[1]\n",
    "\n",
    "    return dense_feat\n",
    "\n",
    "def SPM_features(img, L, k_means_classifier, num_clusters):\n",
    "    W = img.shape[1]\n",
    "    H = img.shape[0]   \n",
    "    h = []\n",
    "    for l in range(L+1):\n",
    "        w_step = math.floor(W/(2**l))\n",
    "        h_step = math.floor(H/(2**l))\n",
    "        x, y = 0, 0\n",
    "        for i in range(1,2**l + 1):\n",
    "            x = 0\n",
    "            for j in range(1, 2**l + 1):                \n",
    "                desc = compute_sift_descriptor_grid(img[y:y+h_step, x:x+w_step])                \n",
    "\n",
    "                predict = k_means_classifier.predict(desc)\n",
    "                histo = np.bincount(predict, minlength=k).reshape(1,-1).ravel()\n",
    "                weight = 2**(l-L)\n",
    "                h.append(weight*histo)\n",
    "                x = x + w_step\n",
    "            y = y + h_step\n",
    "            \n",
    "    hist = np.array(h).ravel()\n",
    "    \n",
    "    # normalize hist\n",
    "    dev = np.std(hist)\n",
    "    hist -= np.mean(hist)\n",
    "    hist /= dev\n",
    "    return hist\n",
    "\n",
    "def get_SPM_histogram(X_train, L, k_means_classifier, num_clusters):\n",
    "    hist_list = []\n",
    "    for img in X_train:\n",
    "        hist = SPM_features(img, L, k_means_classifier, num_clusters)\n",
    "        hist_list.append(hist)\n",
    "    \n",
    "    return np.array(hist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # height, width = img.shape[0], img.shape[1]\n",
    "    \n",
    "    # hist_list = []\n",
    "    # for layer in range(L+1):\n",
    "    #     h = math.floor(height/(2**layer))\n",
    "    #     w = math.floor(width/(2**layer))\n",
    "        \n",
    "    #     for i in range(pow(2, layer)):\n",
    "    #         for j in range(pow(2, layer)):\n",
    "    #             features = compute_sift_descriptor_grid(img[j*h:(j+1)*h , i*h:(i+1)*w])\n",
    "    #             pred = k_means_classifier.predict(features)\n",
    "                \n",
    "    #             hist = np.bincount(pred, minlength = num_clusters).reshape(1,-1).flatten()\n",
    "    #             weight = pow(2,(1-L))\n",
    "    #             weighted_hist = weight*hist\n",
    "    #             hist_list.append(weighted_hist)\n",
    "                \n",
    "    # hist_list = np.array(hist_list).flatten()\n",
    "    \n",
    "    # standard_deviation = np.std(hist_list)\n",
    "    # hist_list -= np.mean(hist_list)\n",
    "    # hist_list /= standard_deviation\n",
    "    \n",
    "    # return hist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Accuracy for BoF: 47.845%\n"
     ]
    }
   ],
   "source": [
    "X_train_hist = get_SPM_histogram(train_img_list, 2, k_means_classifier, num_clusters)\n",
    "X_test_hist = get_SPM_histogram(test_img_list, 2, k_means_classifier, num_clusters)\n",
    "\n",
    "# train SVM\n",
    "clf = LinearSVC(random_state=SEED)\n",
    "clf.fit(X_train_hist, train_label_list)\n",
    "pred = clf.predict(X_test_hist)\n",
    "accuracy = np.mean(pred == test_label_list)*100\n",
    "\n",
    "print(f\"Classification Accuracy for BoF: {accuracy:.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "michael",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
